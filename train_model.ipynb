{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d1572c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c658de2",
   "metadata": {},
   "source": [
    "# Load the data \n",
    "\n",
    "- The original data have 800 cases and 40 variables of dtypes float64(2), int64(17) and object(21).\n",
    "\n",
    "- Deciding features and target:\n",
    "  - features: variables on *the insurance policy*, *demographics of the insured*, *incident details*, *claims*, *automobile* are potentially predictors \n",
    "  - target: `fraud_reported` (N/Y).\n",
    "\n",
    "- Necessary preprocessing:\n",
    "  - `_c39` has only null, drop it, then the only 1 float variable is `policy_annual_premium`. \n",
    "\n",
    "  - Only `capital-gains` and `capital-loss` have hyphen as word link-for consistency, *standardize* column names to \"snake_case\". \n",
    "\n",
    "  - Calculate `policy_age_days` as *the time* between `policy_bind_date` and `incident_date`, because claims filed on very new policies are often suspicious. \n",
    "\n",
    "  - *Drop* some variables that are either unique identifiers, have too many unique values (high cardinality), are irrelevant, or have already had their useful information extracted. \n",
    "\n",
    "  - The *processed data* have 28 variables of dtypes float64(1), int64(15) and object(12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d007345d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 800 entries, 0 to 799\n",
      "Data columns (total 40 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   months_as_customer           800 non-null    int64  \n",
      " 1   age                          800 non-null    int64  \n",
      " 2   policy_number                800 non-null    int64  \n",
      " 3   policy_bind_date             800 non-null    object \n",
      " 4   policy_state                 800 non-null    object \n",
      " 5   policy_csl                   800 non-null    object \n",
      " 6   policy_deductable            800 non-null    int64  \n",
      " 7   policy_annual_premium        800 non-null    float64\n",
      " 8   umbrella_limit               800 non-null    int64  \n",
      " 9   insured_zip                  800 non-null    int64  \n",
      " 10  insured_sex                  800 non-null    object \n",
      " 11  insured_education_level      800 non-null    object \n",
      " 12  insured_occupation           800 non-null    object \n",
      " 13  insured_hobbies              800 non-null    object \n",
      " 14  insured_relationship         800 non-null    object \n",
      " 15  capital-gains                800 non-null    int64  \n",
      " 16  capital-loss                 800 non-null    int64  \n",
      " 17  incident_date                800 non-null    object \n",
      " 18  incident_type                800 non-null    object \n",
      " 19  collision_type               800 non-null    object \n",
      " 20  incident_severity            800 non-null    object \n",
      " 21  authorities_contacted        728 non-null    object \n",
      " 22  incident_state               800 non-null    object \n",
      " 23  incident_city                800 non-null    object \n",
      " 24  incident_location            800 non-null    object \n",
      " 25  incident_hour_of_the_day     800 non-null    int64  \n",
      " 26  number_of_vehicles_involved  800 non-null    int64  \n",
      " 27  property_damage              800 non-null    object \n",
      " 28  bodily_injuries              800 non-null    int64  \n",
      " 29  witnesses                    800 non-null    int64  \n",
      " 30  police_report_available      800 non-null    object \n",
      " 31  total_claim_amount           800 non-null    int64  \n",
      " 32  injury_claim                 800 non-null    int64  \n",
      " 33  property_claim               800 non-null    int64  \n",
      " 34  vehicle_claim                800 non-null    int64  \n",
      " 35  auto_make                    800 non-null    object \n",
      " 36  auto_model                   800 non-null    object \n",
      " 37  auto_year                    800 non-null    int64  \n",
      " 38  fraud_reported               800 non-null    object \n",
      " 39  _c39                         0 non-null      float64\n",
      "dtypes: float64(2), int64(17), object(21)\n",
      "memory usage: 250.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "df = pd.read_csv('data/train_claims.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e85bdd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>months_as_customer</th>\n",
       "      <th>age</th>\n",
       "      <th>policy_number</th>\n",
       "      <th>policy_bind_date</th>\n",
       "      <th>policy_state</th>\n",
       "      <th>policy_csl</th>\n",
       "      <th>policy_deductable</th>\n",
       "      <th>policy_annual_premium</th>\n",
       "      <th>umbrella_limit</th>\n",
       "      <th>insured_zip</th>\n",
       "      <th>...</th>\n",
       "      <th>police_report_available</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>injury_claim</th>\n",
       "      <th>property_claim</th>\n",
       "      <th>vehicle_claim</th>\n",
       "      <th>auto_make</th>\n",
       "      <th>auto_model</th>\n",
       "      <th>auto_year</th>\n",
       "      <th>fraud_reported</th>\n",
       "      <th>_c39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>241</td>\n",
       "      <td>45</td>\n",
       "      <td>596785</td>\n",
       "      <td>2014-03-04</td>\n",
       "      <td>IL</td>\n",
       "      <td>500/1000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1104.50</td>\n",
       "      <td>0</td>\n",
       "      <td>432211</td>\n",
       "      <td>...</td>\n",
       "      <td>NO</td>\n",
       "      <td>91650</td>\n",
       "      <td>14100</td>\n",
       "      <td>14100</td>\n",
       "      <td>63450</td>\n",
       "      <td>Accura</td>\n",
       "      <td>TL</td>\n",
       "      <td>2011</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65</td>\n",
       "      <td>23</td>\n",
       "      <td>876699</td>\n",
       "      <td>1999-12-12</td>\n",
       "      <td>OH</td>\n",
       "      <td>250/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1099.95</td>\n",
       "      <td>0</td>\n",
       "      <td>473109</td>\n",
       "      <td>...</td>\n",
       "      <td>YES</td>\n",
       "      <td>52400</td>\n",
       "      <td>6550</td>\n",
       "      <td>6550</td>\n",
       "      <td>39300</td>\n",
       "      <td>Accura</td>\n",
       "      <td>MDX</td>\n",
       "      <td>2005</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>289</td>\n",
       "      <td>45</td>\n",
       "      <td>943425</td>\n",
       "      <td>1999-10-28</td>\n",
       "      <td>OH</td>\n",
       "      <td>250/500</td>\n",
       "      <td>2000</td>\n",
       "      <td>1221.41</td>\n",
       "      <td>0</td>\n",
       "      <td>466289</td>\n",
       "      <td>...</td>\n",
       "      <td>NO</td>\n",
       "      <td>2700</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>2100</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Accord</td>\n",
       "      <td>2006</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63</td>\n",
       "      <td>26</td>\n",
       "      <td>550930</td>\n",
       "      <td>1995-10-12</td>\n",
       "      <td>IL</td>\n",
       "      <td>500/1000</td>\n",
       "      <td>500</td>\n",
       "      <td>1500.04</td>\n",
       "      <td>6000000</td>\n",
       "      <td>613826</td>\n",
       "      <td>...</td>\n",
       "      <td>YES</td>\n",
       "      <td>5160</td>\n",
       "      <td>860</td>\n",
       "      <td>860</td>\n",
       "      <td>3440</td>\n",
       "      <td>Accura</td>\n",
       "      <td>TL</td>\n",
       "      <td>2004</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>257</td>\n",
       "      <td>43</td>\n",
       "      <td>797636</td>\n",
       "      <td>1992-05-19</td>\n",
       "      <td>IN</td>\n",
       "      <td>100/300</td>\n",
       "      <td>1000</td>\n",
       "      <td>974.84</td>\n",
       "      <td>0</td>\n",
       "      <td>468984</td>\n",
       "      <td>...</td>\n",
       "      <td>YES</td>\n",
       "      <td>85320</td>\n",
       "      <td>21330</td>\n",
       "      <td>7110</td>\n",
       "      <td>56880</td>\n",
       "      <td>Nissan</td>\n",
       "      <td>Pathfinder</td>\n",
       "      <td>2006</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   months_as_customer  age  policy_number policy_bind_date policy_state  \\\n",
       "0                 241   45         596785       2014-03-04           IL   \n",
       "1                  65   23         876699       1999-12-12           OH   \n",
       "2                 289   45         943425       1999-10-28           OH   \n",
       "3                  63   26         550930       1995-10-12           IL   \n",
       "4                 257   43         797636       1992-05-19           IN   \n",
       "\n",
       "  policy_csl  policy_deductable  policy_annual_premium  umbrella_limit  \\\n",
       "0   500/1000               2000                1104.50               0   \n",
       "1    250/500               1000                1099.95               0   \n",
       "2    250/500               2000                1221.41               0   \n",
       "3   500/1000                500                1500.04         6000000   \n",
       "4    100/300               1000                 974.84               0   \n",
       "\n",
       "   insured_zip  ... police_report_available total_claim_amount injury_claim  \\\n",
       "0       432211  ...                      NO              91650        14100   \n",
       "1       473109  ...                     YES              52400         6550   \n",
       "2       466289  ...                      NO               2700          300   \n",
       "3       613826  ...                     YES               5160          860   \n",
       "4       468984  ...                     YES              85320        21330   \n",
       "\n",
       "  property_claim vehicle_claim  auto_make  auto_model auto_year  \\\n",
       "0          14100         63450     Accura          TL      2011   \n",
       "1           6550         39300     Accura         MDX      2005   \n",
       "2            300          2100      Honda      Accord      2006   \n",
       "3            860          3440     Accura          TL      2004   \n",
       "4           7110         56880     Nissan  Pathfinder      2006   \n",
       "\n",
       "  fraud_reported _c39  \n",
       "0              N  NaN  \n",
       "1              Y  NaN  \n",
       "2              N  NaN  \n",
       "3              N  NaN  \n",
       "4              N  NaN  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b992e2",
   "metadata": {},
   "source": [
    "# 1. Splitting Data to Train/Test\n",
    "We do splitting *prior to* any preprocessing and analysis, in order to prevent **data leakage** i.e. using info. in the testing dataset to guide our model training.\n",
    "\n",
    "- training and testing 80/20 split\n",
    "- proportion of fraud cases is roughly the same in both datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82e9149f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully split into training and testing sets.\n",
      "Training set shape: (640, 39)\n",
      "Testing set shape: (160, 39)\n",
      "\n",
      "Fraud distribution in training set:\n",
      "fraud_reported\n",
      "N    0.759375\n",
      "Y    0.240625\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fraud distribution in testing set:\n",
      "fraud_reported\n",
      "N    0.7625\n",
      "Y    0.2375\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define features (X) and target (y)\n",
    "TARGET = 'fraud_reported'\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "# We use stratify=y to ensure the proportion of fraud cases is the same in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Data successfully split into training and testing sets.\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "print(\"\\nFraud distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nFraud distribution in testing set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1166f9e",
   "metadata": {},
   "source": [
    "# 2. Build the Preprocessing and Modeling Pipeline\n",
    "\n",
    "Now, we'll create a pipeline to handle all preprocessing steps. This is the best practice as it prevents data leakage and bundles our entire workflow into a single, reusable object.\n",
    "\n",
    "The pipeline will perform the following steps in order:\n",
    "1.  **Clean and Engineer Features**: A custom transformer will handle replacing `'?'`, creating `policy_age_days`, and dropping unnecessary columns.\n",
    "2.  **Impute Missing Values**: It will fill missing numerical values with the median and categorical values with the mode.\n",
    "3.  **Scale and Encode**: It will apply `StandardScaler` to numerical features and `OneHotEncoder` to categorical features.\n",
    "4.  **Train the Model**: Finally, it will train our `AdaBoostClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5002cc",
   "metadata": {},
   "source": [
    "First, **Create a custom preprocessing step** that can be used inside a `scikit-learn` `Pipeline`. \n",
    "\n",
    "This step handles all the initial data cleaning and feature creation/deletion steps, such as:\n",
    "- Standardizing column names.\n",
    "- Replacing '?' with NaN.\n",
    "- Creating the policy_age_days feature.\n",
    "- Dropping the unnecessary columns.\n",
    "\n",
    "Specifically, create a new class named `FeatureEngineer`, and to make it compatible with `scikit-learn` inherit from two of its built-in classes: \n",
    "   - `BaseEstimator`: the \"base plate\", gives our class methods like `get_params()` and `set_params()`, which work correctly with `GridSearchCV`, so we can have a fine-tuning step\n",
    "   - `TransformerMixin`: a \"helper\" class, we define our own `.fit()` and `.transform()` methods, it gives us a `.fit_transform()` automatically. A convention for any step in a pipeline that transfroms data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65c2a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Define the list of columns to drop\n",
    "# We define this here so it can be used in our custom transformer\n",
    "columns_to_drop = [\n",
    "    'policy_bind_date', 'incident_date', 'policy_number', 'policy_state',\n",
    "    'insured_zip', 'insured_hobbies', 'incident_location', 'incident_city',\n",
    "    'incident_state', 'auto_model', 'auto_year', 'auto_make', '_c39'\n",
    "]\n",
    "\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        # Standardize column names\n",
    "        X_transformed.columns = [col.lower().replace('-', '_') for col in X_transformed.columns]\n",
    "        \n",
    "        # Replace '?' with NaN\n",
    "        X_transformed.replace('?', np.nan, inplace=True)\n",
    "        \n",
    "        # Feature Engineering: policy_age_days\n",
    "        X_transformed['policy_bind_date'] = pd.to_datetime(X_transformed['policy_bind_date'])\n",
    "        X_transformed['incident_date'] = pd.to_datetime(X_transformed['incident_date'])\n",
    "        X_transformed['policy_age_days'] = (X_transformed['incident_date'] - X_transformed['policy_bind_date']).dt.days\n",
    "        \n",
    "        # Drop specified columns\n",
    "        # We use .get(columns_to_drop, []) to avoid errors if columns are already gone\n",
    "        cols_to_drop_in_df = [col for col in columns_to_drop if col in X_transformed.columns]\n",
    "        X_transformed.drop(columns=cols_to_drop_in_df, inplace=True)\n",
    "        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a510af3",
   "metadata": {},
   "source": [
    "**Complete Pipeline Cell**: builds and runs the entire `Pipeline`. This cell:\n",
    "\n",
    "- Defines the preprocessing steps for numerical features (imputation with median, then scaling).\n",
    "- Defines the preprocessing steps for categorical features (imputation with mode, then one-hot encoding).\n",
    "- Combines these steps using `ColumnTransformer`.\n",
    "- Integrates the custom `FeatureEngineer` and the preprocessor into a final `Pipeline` with the `AdaBoostClassifier`.\n",
    "- Trains the entire pipeline on `X_train` and `y_train`.\n",
    "- Evaluates the model on `X_test` and prints the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97061907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the complete model pipeline...\n",
      "Model training complete.\n",
      "\n",
      "Evaluating model performance on the test set...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.88      0.87      0.88       122\n",
      "           Y       0.60      0.63      0.62        38\n",
      "\n",
      "    accuracy                           0.81       160\n",
      "   macro avg       0.74      0.75      0.75       160\n",
      "weighted avg       0.82      0.81      0.81       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Define the initial feature engineering step\n",
    "feature_engineering_step = ('feature_engineering', FeatureEngineer())\n",
    "\n",
    "# Define preprocessing for numerical and categorical features\n",
    "# Note: We are defining this based on expected dtypes AFTER the FeatureEngineer runs.\n",
    "# Let's get the dtypes first by fitting and transforming a sample.\n",
    "temp_df = FeatureEngineer().fit_transform(X_train)\n",
    "numerical_features = temp_df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = temp_df.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Create the preprocessing pipelines\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create the final model pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    feature_engineering_step,\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', AdaBoostClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the complete model pipeline...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nEvaluating model performance on the test set...\")\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618453e",
   "metadata": {},
   "source": [
    "# 3. Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "The initial model provides a good baseline. Now, we'll use `GridSearchCV` to systematically search for the best hyperparameters for our `AdaBoostClassifier`. This will help us fine-tune the model and potentially improve its performance, especially the recall for fraud detection.\n",
    "\n",
    "We will search over:\n",
    "- `n_estimators`: The number of boosting stages to perform.\n",
    "- `learning_rate`: The rate at which the model learns from its mistakes.\n",
    "\n",
    "`GridSearchCV` is very clever. After it finishes cross-validation and finds the best parameters, it *automatically performs one final step*: it retrains the entire pipeline on all of the training data (X_train and y_train) using those best parameters. \n",
    "\n",
    "`grid_search.best_estimator_` is the resulting entire and fully-trained pipeline-we only need to save this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ca3d0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GridSearchCV...\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "\n",
      "GridSearchCV complete.\n",
      "Best parameters found: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 50}\n",
      "Best F1 score on cross-validation: 0.7584\n",
      "\n",
      "Evaluating the best model on the test set...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.91      0.84      0.88       122\n",
      "           Y       0.60      0.74      0.66        38\n",
      "\n",
      "    accuracy                           0.82       160\n",
      "   macro avg       0.75      0.79      0.77       160\n",
      "weighted avg       0.84      0.82      0.82       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search\n",
    "# These are parameters for the 'classifier' step of our pipeline\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "# We use the entire model_pipeline so that the search is done on the preprocessed data\n",
    "# We focus on 'f1_macro' as it's a good overall metric for imbalanced classes\n",
    "# cv=3 means 3-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    model_pipeline, \n",
    "    param_grid, \n",
    "    cv=3, \n",
    "    scoring='f1_macro', \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "print(\"Starting GridSearchCV...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"\\nGridSearchCV complete.\")\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best F1 score on cross-validation: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "print(\"\\nEvaluating the best model on the test set...\")\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f50bb3",
   "metadata": {},
   "source": [
    "# 4. Save the Best Model\n",
    "\n",
    "Now that we have a well-performing, tuned model, the final step is to save it to a file. This process, called **serialization**, allows us to load the entire pipeline—including the feature engineering, preprocessing, and the trained classifier—into another environment (like a web application or an API) to make predictions on new, unseen data without having to retrain it.\n",
    "\n",
    "We will use the `joblib` library, which is efficient for saving scikit-learn models. Here we save `best_model`, the entire, fitted `Pipeline` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d85fe56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pipeline saved successfully to: models/fraud_detection_pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "# Define the file path for the saved model\n",
    "model_path = 'models/fraud_detection_pipeline.pkl'\n",
    "\n",
    "# Save the best_model pipeline using joblib\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "print(f\"Model pipeline saved successfully to: {model_path}\")\n",
    "\n",
    "# You can also load it back to verify\n",
    "# loaded_model = joblib.load(model_path)\n",
    "# print(\"\\nModel loaded successfully.\")\n",
    "# print(loaded_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
